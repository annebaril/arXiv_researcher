{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ArxivRAG\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the JSON file\n",
    "# df = spark.read.json(\"../data/reduce.json\")\n",
    "df = spark.read.json(\"../data/arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "# Show schema\n",
    "# df.printSchema()\n",
    "\n",
    "print(df.count())\n",
    "\n",
    "# Show some sample rows\n",
    "# df.select(\"id\", \"title\", \"abstract\", \"categories\", \"update_date\", \"versions\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the JSON file\n",
    "# df = spark.read.json(\"../arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "# Show schema\n",
    "# df.printSchema()\n",
    "\n",
    "df.describe().show()\n",
    "# print(df.count())\n",
    "# df_copy = df.limit(10000)\n",
    "# df_copy.write.format(\"json\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save(\"test3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lower, regexp_replace, trim\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "\n",
    "\n",
    "# Combine title and abstract into a 'document' field\n",
    "df_cleaned = df.select(\n",
    "    \"id\", \"title\", \"abstract\", \"versions\", \"authors\"\n",
    ").withColumn(\n",
    "    \"document\",\n",
    "    concat_ws(\" \", col(\"title\"), col(\"abstract\"))\n",
    ").withColumn(\n",
    "    \"document\",\n",
    "    lower(regexp_replace(col(\"document\"), r\"[^a-zA-Z0-9\\s]\", \"\"))\n",
    ").withColumn(\n",
    "    \"document\", trim(col(\"document\"))\n",
    ").withColumn(\n",
    "    \"row_id\", monotonically_increasing_id()\n",
    ").withColumn(\n",
    "    \"year\", col(\"versions\")[0][\"created\"].substr(-17, 4)\n",
    ")\n",
    "print(df_cleaned.count())\n",
    "# Filter out empty documents\n",
    "# df_cleaned = df_cleaned.filter(col(\"document\") != \"\")\n",
    "# print(df_cleaned.count())\n",
    "df_cleaned = df_cleaned.filter(col(\"abstract\") is not None)\n",
    "print(df_cleaned.count())\n",
    "# df_cleaned.groupBy(\"year\").count().orderBy(\"count\").show(100)\n",
    "# Optional: Sample 10,000 rows for development\n",
    "# df_sample = df_cleaned.limit(1)\n",
    "\n",
    "# df_sample = df_cleaned.filter((col(\"year\") == 1988) | (col(\"year\") == 1986)|(col(\"year\") == 1989)|(col(\"year\") == 1990)|(col(\"year\") == 1991)|(col(\"year\") == 1992))\n",
    "\n",
    "# Show a few processed rows\n",
    "# df_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleaned.select(\"year\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned.describe().show()\n",
    "# Show the number of unique categories\n",
    "df_cleaned.select(\"year\").distinct().count()\n",
    "df_cleaned.select(\"year\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = df_sample.select(\"id\", \"document\", \"year\", \"title\", \"authors\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.shape\n",
    "# pandas_df[pandas_df[\"year\"].apply(lambda x: int(x) > 1960 and int(x) < 2026)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='34.163.106.5', port=8000)\n",
    "chroma_client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the documents\n",
    "# embeddings = model.encode(pandas_df['document'].tolist(), show_progress_bar=True)\n",
    "\n",
    "metadata = [{\"id\": row[\"id\"], \"year\": row[\"year\"], \"title\": row[\"title\"], \"authors\": row[\"authors\"]} for _, row in pandas_df.iterrows()]\n",
    "# print(metadata, \"test\")\n",
    "vectorstore = Chroma.from_texts(pandas_df['document'].tolist(),embedding=embeddings,metadatas = metadata, ids=pandas_df['id'].tolist(), client=chroma_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_cleaned.filter(col(\"row_id\") >= 1020).drop(\"row_id\").limit(1)\n",
    "\n",
    "# # Show a few processed rows\n",
    "df_sample.select(\"id\", \"categories\", \"document\", \"year\", \"title\", \"authors\").show(5, truncate=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = df_sample.select(\"id\", \"document\", \"year\", \"title\", \"authors\").toPandas()\n",
    "\n",
    "\n",
    "metadata = [{\"id\": row[\"id\"], \"year\": row[\"year\"], \"title\": row[\"title\"], \"authors\": row[\"authors\"]} for _, row in pandas_df.iterrows()]\n",
    "vectorstore.add_texts(pandas_df['document'].tolist(),embedding=embeddings,metadatas = metadata, ids=pandas_df['id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# test query\n",
    "query = \"field\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "i = 0\n",
    "for doc in retrieved_docs:\n",
    "    print(i, doc.page_content)\n",
    "    print(\"\\n\")\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "BUCKET_NAME = \"arxiv-researcher-bucket\"\n",
    "GCS_PERSIST_PATH = \"chroma_db/\"\n",
    "LOCAL_PERSIST_PATH = \"./local_chromadb/\"\n",
    "\n",
    "REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blobs = bucket.list_blobs(prefix=GCS_PERSIST_PATH)\n",
    "\n",
    "# Download Chroma persisted data from GCS to local directory\n",
    "for blob in blobs:\n",
    "    print(blob.name)\n",
    "    # if not blob.name.endswith(\"/\"):  # Avoid directory blobs\n",
    "    #     relative_path = os.path.relpath(blob.name, GCS_PERSIST_PATH)\n",
    "    #     local_file_path = os.path.join(LOCAL_PERSIST_PATH, relative_path)\n",
    "    #     os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    #     blob.download_to_filename(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='34.163.92.97', port=8000)\n",
    "chroma_client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "today = datetime.today()\n",
    "print(today)\n",
    "# now = datetime.datetime.now()\n",
    "d = today - timedelta(days=7)\n",
    "print(d)\n",
    "today = '%04d%02d%02d0600' % (today.year, today.month, today.day)\n",
    "d='%04d%02d%02d0600' % (d.year, d.month, d.day)\n",
    "print(f\"submittedDate:[{d}+TO+{today}]\")\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "# search = arxiv.Search(\n",
    "#   query = \"quantum\",\n",
    "#   max_results = 10,\n",
    "#   sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "# )\n",
    "\n",
    "# results = client.results(search)\n",
    "\n",
    "# # `results` is a generator; you can iterate over its elements one by one...\n",
    "# for r in client.results(search):\n",
    "#   print(r.title)\n",
    "# # ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "# all_results = list(results)\n",
    "# print([r.title for r in all_results])\n",
    "\n",
    "# For advanced query syntax documentation, see the arXiv API User Manual:\n",
    "# https://arxiv.org/help/api/user-manual#query_details\n",
    "# submittedDate:[202301010600+TO+202401010600]\n",
    "search = arxiv.Search(query = f\"submittedDate:[{d} TO {today}]\")\n",
    "results = client.results(search)\n",
    "results = [\n",
    "    {\n",
    "        \"title\": r.title,\n",
    "        \"year\": r.updated.year,\n",
    "        \"authors\": \", \".join(a.name for a in r.authors),\n",
    "        \"summary\": r.summary,\n",
    "        \"id\": r.entry_id.split(\"abs/\")[-1],  # 👈 prend l’ID complet (avec catégorie si elle y est)\n",
    "    }\n",
    "    for r in client.results(search)\n",
    "]\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : transformer en DataFrame Spark\n",
    "df = spark.createDataFrame(results)\n",
    "\n",
    "df_cleaned = df.select(\n",
    "    \"id\", \"title\", \"summary\", \"authors\", \"year\"\n",
    ").withColumn(\n",
    "    \"document\",\n",
    "    concat_ws(\" \", col(\"title\"), col(\"summary\"))\n",
    ").withColumn(\n",
    "    \"document\",\n",
    "    lower(regexp_replace(col(\"document\"), r\"[^a-zA-Z0-9\\s]\", \"\"))\n",
    ").withColumn(\n",
    "    \"document\", trim(col(\"document\"))\n",
    ")\n",
    "\n",
    "# (Optionnel) afficher les données\n",
    "df_cleaned.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df_cleaned.select(\"id\", \"document\", \"year\", \"title\", \"authors\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='34.163.106.5', port=8000)\n",
    "chroma_client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata = [{\"id\": row[\"id\"], \"year\": row[\"year\"], \"title\": row[\"title\"], \"authors\": row[\"authors\"]} for _, row in pandas_df.iterrows()]\n",
    "# print(metadata, \"test\")\n",
    "vectorstore = Chroma.from_texts(pandas_df['document'].tolist(),embedding=embeddings,metadatas = metadata, ids=pandas_df['id'].tolist(), client=chroma_client)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
