{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ArxivRAG\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the JSON file\n",
    "df = spark.read.json(\"reduce.json\")\n",
    "\n",
    "# Show schema\n",
    "# df.printSchema()\n",
    "\n",
    "# print(df.count())\n",
    "\n",
    "\n",
    "# Show some sample rows\n",
    "# df.select(\"id\", \"title\", \"abstract\", \"categories\", \"update_date\", \"versions\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the JSON file\n",
    "# df = spark.read.json(\"../arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "# Show schema\n",
    "# df.printSchema()\n",
    "\n",
    "# print(df.count())\n",
    "# df_copy = df.limit(10000)\n",
    "# df_copy.write.format(\"json\") \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save(\"test3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lower, regexp_replace, trim\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "\n",
    "\n",
    "# Combine title and abstract into a 'document' field\n",
    "df_cleaned = df.select(\n",
    "    \"id\", \"title\", \"abstract\", \"categories\", \"versions\", \"title\", \"authors\"\n",
    ").withColumn(\n",
    "    \"document\",\n",
    "    concat_ws(\" \", col(\"title\"), col(\"abstract\"))\n",
    ").withColumn(\n",
    "    \"document\",\n",
    "    lower(regexp_replace(col(\"document\"), r\"[^a-zA-Z0-9\\s]\", \"\"))\n",
    ").withColumn(\n",
    "    \"document\", trim(col(\"document\"))\n",
    ").withColumn(\n",
    "    \"row_id\", monotonically_increasing_id()\n",
    ").withColumn(\n",
    "    \"year\", col(\"versions\")[0][\"created\"].substr(-17, 4)\n",
    ")\n",
    "# Filter out empty documents\n",
    "df_cleaned = df_cleaned.filter(col(\"document\") != \"\")\n",
    "\n",
    "# Optional: Sample 10,000 rows for development\n",
    "df_sample = df_cleaned.limit(1)\n",
    "# Show a few processed rows\n",
    "df_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = df_sample.select(\"id\", \"document\", \"year\", \"title\", \"authors\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.shape\n",
    "# pandas_df[pandas_df[\"year\"].apply(lambda x: int(x) > 1960 and int(x) < 2026)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the documents\n",
    "# embeddings = model.encode(pandas_df['document'].tolist(), show_progress_bar=True)\n",
    "\n",
    "metadata = [{\"id\": row[\"id\"], \"year\": row[\"year\"], \"title\": row[\"title\"], \"authors\": row[\"authors\"]} for _, row in pandas_df.iterrows()]\n",
    "# print(metadata, \"test\")\n",
    "vectorstore = Chroma.from_texts(pandas_df['document'].tolist(),embedding=embeddings,metadatas = metadata, ids=pandas_df['id'].tolist() ,persist_directory=\"../chroma_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_cleaned.filter(col(\"row_id\") >= 1020).drop(\"row_id\").limit(1)\n",
    "\n",
    "# # Show a few processed rows\n",
    "df_sample.select(\"id\", \"categories\", \"document\", \"year\", \"title\", \"authors\").show(5, truncate=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = df_sample.select(\"id\", \"document\", \"year\", \"title\", \"authors\").toPandas()\n",
    "\n",
    "\n",
    "metadata = [{\"id\": row[\"id\"], \"year\": row[\"year\"], \"title\": row[\"title\"], \"authors\": row[\"authors\"]} for _, row in pandas_df.iterrows()]\n",
    "vectorstore.add_texts(pandas_df['document'].tolist(),embedding=embeddings,metadatas = metadata, ids=pandas_df['id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# test query\n",
    "query = \"field\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "i = 0\n",
    "for doc in retrieved_docs:\n",
    "    print(i, doc.page_content)\n",
    "    print(\"\\n\")\n",
    "    i +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
