{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, RetrievalQA\n",
    "from langchain.chat_models import ChatHuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_directory_from_gcs(gcs_directory, local_directory, bucket_name):\n",
    "    \"\"\"Download all files from a GCS directory to a local directory.\"\"\"\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=gcs_directory)\n",
    "\n",
    "    for blob in blobs:\n",
    "        if not blob.name.endswith(\"/\"):  # Avoid directory blobs\n",
    "            relative_path = os.path.relpath(blob.name, gcs_directory)\n",
    "            local_file_path = os.path.join(local_directory, relative_path)\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "            blob.download_to_filename(local_file_path)\n",
    "            print(f\"Downloaded {blob.name} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "GCS_PERSIST_PATH = os.getenv(\"GCS_PERSIST_PATH\")\n",
    "LOCAL_PERSIST_PATH = os.getenv(\"LOCAL_PERSIST_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv-researcher-bucket'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/data_level0.bin to ../chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/data_level0.bin\n",
      "Downloaded chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/header.bin to ../chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/header.bin\n",
      "Downloaded chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/index_metadata.pickle to ../chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/index_metadata.pickle\n",
      "Downloaded chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/length.bin to ../chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/length.bin\n",
      "Downloaded chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/link_lists.bin to ../chroma_db/4d8e52f2-6027-41ef-b06b-77f96cb894fa/link_lists.bin\n",
      "Downloaded chroma_db/chroma.sqlite3 to ../chroma_db/chroma.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# Download Chroma persisted data from GCS to local directory\n",
    "download_directory_from_gcs(GCS_PERSIST_PATH, LOCAL_PERSIST_PATH, BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celiazhang/.pyenv/versions/3.10.12/envs/arxivenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_16499/2072962191.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=LOCAL_PERSIST_PATH, embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Load the stored vector database\n",
    "db = Chroma(persist_directory=LOCAL_PERSIST_PATH, embedding_function=embeddings)\n",
    "\n",
    "# Now use db for retrieval\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Neural networks for image recognition\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "comparing robustness of pairwise and multiclass neuralnetwork systems\n",
      "  for face recognition   noise corruptions and variations in face images can seriously hurt the\n",
      "performance of face recognition systems to make such systems robust\n",
      "multiclass neuralnetwork classifiers capable of learning from noisy data have\n",
      "been suggested however on large face data sets such systems cannot provide the\n",
      "robustness at a high level in this paper we explore a pairwise neuralnetwork\n",
      "system as an alternative approach to improving the robustness of face\n",
      "recognition in our experiments this approach is shown to outperform the\n",
      "multiclass neuralnetwork system in terms of the predictive accuracy on the\n",
      "face images corrupted by noise\n",
      "\n",
      "{'id': '0704.3515', 'year': '2007'}\n",
      "\n",
      "\n",
      "2\n",
      "the parameterless selforganizing map algorithm   the parameterless selforganizing map plsom is a new neural network\n",
      "algorithm based on the selforganizing map som it eliminates the need for a\n",
      "learning rate and annealing schemes for learning rate and neighbourhood size\n",
      "we discuss the relative performance of the plsom and the som and demonstrate\n",
      "some tasks in which the som fails but the plsom performs satisfactory finally\n",
      "we discuss some example applications of the plsom and present a proof of\n",
      "ordering under certain limited conditions\n",
      "\n",
      "{'id': '0705.0199', 'year': '2007'}\n",
      "\n",
      "\n",
      "3\n",
      "exploiting heavy tails in training times of multilayer perceptrons a\n",
      "  case study with the uci thyroid disease database   the random initialization of weights of a multilayer perceptron makes it\n",
      "possible to model its training process as a las vegas algorithm ie a\n",
      "randomized algorithm which stops when some required training error is obtained\n",
      "and whose execution time is a random variable this modeling is used to perform\n",
      "a case study on a wellknown pattern recognition benchmark the uci thyroid\n",
      "disease database empirical evidence is presented of the training time\n",
      "probability distribution exhibiting a heavy tail behavior meaning a big\n",
      "probability mass of long executions this fact is exploited to reduce the\n",
      "training time cost by applying two simple restart strategies the first assumes\n",
      "full knowledge of the distribution yielding a 40 cut down in expected time\n",
      "with respect to the training without restarts the second assumes null\n",
      "knowledge yielding a reduction ranging from 9 to 23\n",
      "\n",
      "{'id': '0704.2725', 'year': '2007'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for doc in retrieved_docs:\n",
    "    print(i)\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print(\"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatHuggingFace\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "/home/celiazhang/.pyenv/versions/3.10.12/envs/arxivenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The FIFA World Cup is a soccer (or football, depending on your preference) tournament that takes place every four years. The 1994 World Cup was indeed held in the United States. Now, let's find out who won it.\n",
      "\n",
      "The 1994 FIFA World Cup final was played on July 17, 1994. The two teams that made it to the final were Brazil and Italy. Brazil won the match 0-0 after extra time, and then 3-2 in a penalty shootout.\n",
      "\n",
      "So, the answer is Brazil. They won the FIFA World Cup in the year 1994.\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "# llm_chain = prompt | llm\n",
    "# print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celiazhang/.pyenv/versions/3.10.12/envs/arxivenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Which articles use LLM in Finance, and return back the id and metadata of this article', 'result': \" To answer this question, we would need to use a combination of access log data and citation information. The article investigating the use of access logs for paper recommendations on arXiv mentions finance as one of the fields of physics it covers. However, we cannot directly determine which specific articles in finance are being referred to without further investigation. We could look for articles in the arXiv finance category that have high access frequencies or use coaccess measures to identify related papers. Additionally, we could check the citation information of papers in the finance category to see which ones have a high number of citations in the finance literature. An online recommendation system has been built based on this research to help scientists find further relevant literature, so we could potentially use this system to identify articles in finance that have been frequently accessed or recommended.\\n\\nUnhelpful Answer: I'm sorry, I don't have access to the specific articles you're asking about, but I can tell you that LLM is a term used in finance to refer to Limited Liability Master Limited Partnerships. However, I cannot determine which articles use this term specifically without further information.\"}\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Which articles use LLM in Finance, and return back the id and metadata of this article\"\n",
    "response1 = qa_chain.invoke(query1)\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celiazhang/.pyenv/versions/3.10.12/envs/arxivenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'les articles dans cette base de données concernent quels domains?', 'result': ' These articles in this database concern mathematics and the statistical properties of European universities in the context of the science system. Specifically, they discuss scaling rules, such as the sizedependent cumulative advantage for citations and the relation between journal impact and field citation density. They also compare typological profiles of languages.'}\n"
     ]
    }
   ],
   "source": [
    "query2 = \"les articles dans cette base de données concernent quels domains?\"\n",
    "response2 = qa_chain.invoke(query2)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
