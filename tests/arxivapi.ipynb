{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arxiv\n",
    "from os import environ\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(query, max_results=10):\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = []\n",
    "    for result in client.results(search):\n",
    "        results.append({\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary,\n",
    "            \"authors\": [a.name for a in result.authors],\n",
    "            \"url\": result.entry_id\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arxiv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m papers \u001b[38;5;241m=\u001b[39m \u001b[43msearch_arxiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLLM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m papers:\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36msearch_arxiv\u001b[0;34m(query, max_results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch_arxiv\u001b[39m(query, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43marxiv\u001b[49m\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m      3\u001b[0m     search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(\n\u001b[1;32m      4\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m      5\u001b[0m         max_results\u001b[38;5;241m=\u001b[39mmax_results,\n\u001b[1;32m      6\u001b[0m         sort_by\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortCriterion\u001b[38;5;241m.\u001b[39mRelevance\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arxiv' is not defined"
     ]
    }
   ],
   "source": [
    "papers = search_arxiv(\"LLM\", 10)\n",
    "i = 0\n",
    "for paper in papers:\n",
    "    print(i, paper[\"title\"], paper[\"url\"])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(openai_api_key)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for paper in papers:\n",
    "    content = f\"title: {paper['title']} \\n Summary: {paper['summary']}, authors: {paper['authors']}, url: {paper['url']}\"\n",
    "    docs.append(content)\n",
    "\n",
    "vectorstore = Chroma.from_texts(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation \n",
      " Summary: LLM self-evaluation relies on the LLM's own ability to estimate response\n",
      "correctness, which can greatly improve its deployment reliability. In this\n",
      "research track, we propose the Chain-of-Embedding (CoE) in the latent space to\n",
      "enable LLMs to perform output-free self-evaluation. CoE consists of all\n",
      "progressive hidden states produced during the inference time, which can be\n",
      "treated as the latent thinking path of LLMs. We find that when LLMs respond\n",
      "correctly and incorrectly, their CoE features differ, these discrepancies\n",
      "assist us in estimating LLM response correctness. Experiments in four diverse\n",
      "domains and seven LLMs fully demonstrate the effectiveness of our method.\n",
      "Meanwhile, its label-free design intent without any training and\n",
      "millisecond-level computational cost ensures real-time feedback in large-scale\n",
      "scenarios. More importantly, we provide interesting insights into LLM response\n",
      "correctness from the perspective of hidden state changes inside LLMs.\n",
      "\n",
      "\n",
      "title: LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked \n",
      " Summary: Large language models (LLMs) are popular for high-quality text generation but\n",
      "can produce harmful content, even when aligned with human values through\n",
      "reinforcement learning. Adversarial prompts can bypass their safety measures.\n",
      "We propose LLM Self Defense, a simple approach to defend against these attacks\n",
      "by having an LLM screen the induced responses. Our method does not require any\n",
      "fine-tuning, input preprocessing, or iterative output generation. Instead, we\n",
      "incorporate the generated content into a pre-defined prompt and employ another\n",
      "instance of an LLM to analyze the text and predict whether it is harmful. We\n",
      "test LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent\n",
      "LLMs against various types of attacks, such as forcefully inducing affirmative\n",
      "responses to prompts and prompt engineering attacks. Notably, LLM Self Defense\n",
      "succeeds in reducing the attack success rate to virtually 0 using both GPT 3.5\n",
      "and Llama 2. The code is publicly available at\n",
      "https://github.com/poloclub/llm-self-defense\n",
      "\n",
      "\n",
      "title: LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity \n",
      " Summary: Combining large language models during training or at inference time has\n",
      "shown substantial performance gain over component LLMs. This paper presents\n",
      "LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique\n",
      "properties: (i) We introduce the focal diversity metric to capture the\n",
      "diversity-performance correlation among component LLMs of an ensemble. (ii) We\n",
      "develop a diversity-optimized ensemble pruning algorithm to select the top-k\n",
      "sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends\n",
      "top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii)\n",
      "We generate new output for each prompt query by utilizing a learn-to-ensemble\n",
      "approach, which learns to detect and resolve the output inconsistency among all\n",
      "component LLMs of an ensemble. Extensive evaluation on four different\n",
      "benchmarks shows good performance gain over the best LLM ensemble methods: (i)\n",
      "In constrained solution set problems, LLM-TOPLA outperforms the best-performing\n",
      "ensemble (Mixtral) by 2.2\\% in accuracy on MMLU and the best-performing LLM\n",
      "ensemble (MoreAgent) on GSM8k by 2.1\\%. (ii) In generative tasks, LLM-TOPLA\n",
      "outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by\n",
      "$3.9\\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and\n",
      "dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available\n",
      "at https://github.com/git-disl/llm-topla\n",
      "\n",
      "\n",
      "title: LLM-as-a-tutor in EFL Writing Education: Focusing on Evaluation of Student-LLM Interaction \n",
      " Summary: In the context of English as a Foreign Language (EFL) writing education,\n",
      "LLM-as-a-tutor can assist students by providing real-time feedback on their\n",
      "essays. However, challenges arise in assessing LLM-as-a-tutor due to differing\n",
      "standards between educational and general use cases. To bridge this gap, we\n",
      "integrate pedagogical principles to assess student-LLM interaction. First, we\n",
      "explore how LLMs can function as English tutors, providing effective essay\n",
      "feedback tailored to students. Second, we propose three metrics to evaluate\n",
      "LLM-as-a-tutor specifically designed for EFL writing education, emphasizing\n",
      "pedagogical aspects. In this process, EFL experts evaluate the feedback from\n",
      "LLM-as-a-tutor regarding quality and characteristics. On the other hand, EFL\n",
      "learners assess their learning outcomes from interaction with LLM-as-a-tutor.\n",
      "This approach lays the groundwork for developing LLMs-as-a-tutor tailored to\n",
      "the needs of EFL learners, advancing the effectiveness of writing education in\n",
      "this context.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# test query\n",
    "query = \"LLM\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
